An Introduction to GLM Theory
========================================================
author: Michael Nielsen
date: October 2017
autosize: true

Goals of Modeling
========================================================

- *Understanding* the relationships between variables

- *Estimating* unkown parameters

- *Predicting* the value of one variable, given a set of known predictor variables

Definitions
========================================================

$Y =$ Response Variable
* aka Target Variable, Dependent Variable
 
$X_1, X_2, ... =$ Predictor Variables  
* aka Explanatory Variables, Independent Variables  
* Factor - a categorical predictor  
  + Dummy Variable - an indicator to represent the presence/absence of a categorical variable  
* Covariate/Variate - a continuous predictor  
    
Covariate Profile

Understanding Dependence
========================================================

Scatterplots

***

```{r, echo=F}
set.seed(12345)

a <- 2.32
b <- -1.86

x <- rnorm(100,5,1)
y <- a + b*x + rnorm(100,0,1.5)

plot(x,y, bty='n')
```

Understanding Dependence
========================================================

Scatterplots

Correlation

```{r}
cor(x,y)
```

***

```{r, echo=F}
plot(x,y, bty='n')
```

Understanding Dependence
========================================================

Scatterplots

Correlation

```{r}
cor(x,y)
```

Mean Functions

$$
  \begin{aligned}
    E(Y|X=x) &= f(x) \\
             &= 2.32 - 1.86x
  \end{aligned}
$$

***

```{r, echo=F}
slr <- lm(y~x)

plot(x,y, bty='n')
abline(a=slr$coefficients[1], b=slr$coefficients[2], col='red')

```


Ordinary Least Squares
========================================================

Minimize the total squared error

$$
  \sum_{i=1}^n [y_i -(\beta_0 + \beta_1 x_i)]^2 \\
  = \sum_{i=1}^n (y_i - \hat{y}_i)^2
$$

***

```{r, echo=F}
x <- c(2, 4, 5)
y <- c(-3, -1, -9)
slr <- lm(y~x)
y.hat <- fitted(slr)

plot(c(1,6),c(2,-10),xlab='', ylab='', type='n', bty='n')
  points(x, y)
  abline(a=slr$coef[1], b=slr$coef[2], col='red')
  segments(x0=x, y0=y, x1=x, y1=y.hat, lty=2)

```

What kind of models are linear?
========================================================

Generic linear model:

$$
\begin{aligned}
  y &= \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p \\
    &= \textbf{x}_i^T \beta
\end{aligned}
$$

Examples:
$$
\begin{aligned}
  y &= \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 \\
  y &= \beta_0 \beta_1^{x_1} \beta_2^{x_2} \implies
    ln(y) = ln(\beta_0) + x_1 ln(\beta_1) + + x_2 ln(\beta_2) \\
  y &= \beta_1 e^{\beta_2 x_2} + \beta_3 e^{\beta_4 x_4} \implies
    \text{not linear}
\end{aligned}
$$

$$
ln(\text{Pure Premium}) = \text{base} + \beta_1 \text{Cov A} +
  \beta_2 \text{Tier} +
  \cdots
$$

Ordinary Least Squares
========================================================


Advantages
* Only one assumption -- The mean model has a known linear form
* Solve for parameters using calculus methods
* Simple, closed form solution

Disadvantages
* A mathematical method, not a statistical method
* Can't make inferences without a probability distribution

Normal Distribution
========================================================

```{r, echo=F, fig.show='hold'}
norm.plot <- function(var){
  plot(seq(5,35,.1), dnorm(seq(5,35,.1), 20, 1), type='n', xlab='', ylab='')
  lines(seq(5,35,.1), dnorm(seq(5,35,.1), 20, sqrt(var)),col='red')
}

norm.plot(20)
norm.plot(5)
norm.plot(1)
```

Binomial Proportion Distribution
========================================================
$X=\frac{0}{n}, \frac{1}{n}, \frac{2}{n}, ... , 1$  
  $n=\text{Number of trials}$  
  $\pi = P(\text{Success})$  


$E(X) = \pi$  
$var(X) = \frac{\pi(1-\pi)}{n}$  

***

```{r, echo=F}
bin.plot <- function(n1){
  plot(seq(0,n1,1)/n1, 
       dbinom(seq(0,n1,1),n1,.2),
       type='h',
       col='red',
       xlab='', ylab='', bty='n')
  title(main=paste('n = ', n1))
}

bin.plot(5)
```

Binomial Proportion Distribution
========================================================

***

```{r, echo=F}
bin.plot(10)
```

Binomial Proportion Distribution
========================================================

***

```{r, echo=F}
bin.plot(20)
```

Binomial Proportion Distribution
========================================================

***

```{r, echo=F}
bin.plot(40)
  lines(seq(0,40,.01)/40,
        dnorm(seq(0,40,.01),8,sqrt(40*.2*.8)),
        col='blue')
```

Poisson Distribution
========================================================

$X = 0, 1, 2, ...$   
$E(X)=\lambda$  
$var(X) = \lambda$  

***
```{r, echo=F}
poi.plot <- function(m){
  plot(seq(0,20,1),
       dpois(seq(0,20,1), m),
       type='h', col='red', bty='n', xlab='', ylab='')
  title(main=paste('Poisson (mean = ',m,')', sep=''))
}

poi.plot(.5)

```

Poisson Distribution
========================================================
```{r, echo=F}
poi.plot(2)
```

***

```{r, echo=F}
poi.plot(5)
```

Poisson Distribution
========================================================
```{r, echo=F}
poi.plot(10)
  lines(seq(0,20,.001),
    dnorm(seq(0,20,.001),10 , sqrt(10)),
    type='l', col='blue')
```

Gamma Distribution
========================================================
```{r, echo=F}
gam.plot<-function(a){
  plot(seq(0,80,.001),
       dgamma(seq(0,80,.001),shape=10, scale=20/10),
       type='n', xlab='', ylab='', bty='n')
    lines(seq(0,80,.001),
          dgamma(seq(0,80,.001),shape=a, scale=20/a),
          col='red')
    title(main=bquote(alpha==.(a)))
}

gam.plot(1)
```


Gamma Distribution
========================================================
```{r, echo=F}
gam.plot(2)
```

Gamma Distribution
========================================================
```{r, echo=F}
gam.plot(4)
```

Gamma Distribution
========================================================
```{r, echo=F}
gam.plot(10)
lines(seq(0,80,.001),
      dnorm(seq(0,80,.001),20, sqrt(40)),
      col='blue')
```


Normal Linear Model
========================================================

Assumptions:  

$$
  y_i \gets Y_i \: indep \sim N(\mu_i, \sigma^2) \\
  \mu_i = x_i^T \beta
$$

Alternative specification:

$$
  y_i \gets Y_i = x_i^T \beta + E_i \\
  E_i \: indep \sim N(0, \sigma^2)
$$

Normal Linear Model - Solving for coefficients
========================================================

## Maximum Likelihood Estimates (MLS)
* Select parameter values that are "most likely" to produce the observed data

## Least Squares Estimates (LSE)
* Under normality assumption, LSE = MLE
* Even without normality, LSE has lowest variance among all possible linear unbiased estimators

Generalized Linear Model
========================================================

Assumptions:

$$
  y_i \gets Y_i \: indep \sim EF(\mu_i, \phi) \\
  g(\mu_i) = x_i^T \beta
$$

* Exponential Family
* Link Function
* Linear Predictor

GLM: Exponential Family
========================================================

$$
f(y_i; \theta_i, \phi)=\text{exp} \left[ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) \right]
$$

Examples:
* Normal
* Gamma
* Poisson
* Binomial

